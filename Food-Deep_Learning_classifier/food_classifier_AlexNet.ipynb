{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convolution Network -----> food classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Adapted notebook from Author: Aymeric Damien\n",
    "# Project: https://github.com/aymericdamien/TensorFlow-Examples/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#util and imports\n",
    "%matplotlib inline\n",
    "\n",
    "import os\n",
    "import fnmatch\n",
    "from cStringIO import StringIO\n",
    "import numpy as np\n",
    "import PIL.Image\n",
    "import IPython.display\n",
    "import shutil\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "def list_all_files(directory, extensions=None):\n",
    "    for root, dirnames, filenames in os.walk(directory):\n",
    "        for filename in filenames:\n",
    "            base, ext = os.path.splitext(filename)\n",
    "            joined = os.path.join(root, filename)\n",
    "            if extensions is None or ext.lower() in extensions:\n",
    "                yield joined\n",
    "                \n",
    "def show_array(a, fmt='png', filename=None):\n",
    "    a = np.uint8(np.clip(a, 0, 255))\n",
    "    image_data = StringIO()\n",
    "    PIL.Image.fromarray(a).save(image_data, fmt)\n",
    "    if filename is None:\n",
    "        IPython.display.display(IPython.display.Image(data=image_data.getvalue()))\n",
    "    else:\n",
    "        with open(filename, 'w') as f:\n",
    "            image_data.seek(0)\n",
    "            shutil.copyfileobj(image_data, f)\n",
    "\n",
    "def find_rectangle(n, max_ratio=2):\n",
    "    sides = []\n",
    "    square = int(math.sqrt(n))\n",
    "    for w in range(square, max_ratio * square):\n",
    "        h = n / w\n",
    "        used = w * h\n",
    "        leftover = n - used\n",
    "        sides.append((leftover, (w, h)))\n",
    "    return sorted(sides)[0][1]\n",
    "\n",
    "# should work for 1d and 2d images, assumes images are square but can be overriden\n",
    "def make_mosaic(images, n=None, nx=None, ny=None, w=None, h=None):\n",
    "    if n is None and nx is None and ny is None:\n",
    "        nx, ny = find_rectangle(len(images))\n",
    "    else:\n",
    "        nx = n if nx is None else nx\n",
    "        ny = n if ny is None else ny\n",
    "    images = np.array(images)\n",
    "    if images.ndim == 2:\n",
    "        side = int(np.sqrt(len(images[0])))\n",
    "        h = side if h is None else h\n",
    "        w = side if w is None else w\n",
    "        images = images.reshape(-1, h, w)\n",
    "    else:\n",
    "        h = images.shape[1]\n",
    "        w = images.shape[2]\n",
    "    image_gen = iter(images)\n",
    "    mosaic = np.empty((h*ny, w*nx))\n",
    "    for i in range(ny):\n",
    "        ia = (i)*h\n",
    "        ib = (i+1)*h\n",
    "        for j in range(nx):\n",
    "            ja = j*w\n",
    "            jb = (j+1)*w\n",
    "            mosaic[ia:ib, ja:jb] = next(image_gen)\n",
    "    return mosaic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 6416 exemples d'arròs\n",
      "loaded 7078 exemples de carn\n",
      "loaded 5495 exemples d'entrepans\n",
      "loaded 6474 exemples de pasta\n",
      "loaded 3165 exemples de pizza\n",
      "loaded 9171 exemples de sopa\n",
      "loaded 6199 exemples de sushi\n",
      "loaded 9290 exemples de verd\n"
     ]
    }
   ],
   "source": [
    "arros = list(list_all_files('../img_classifier/arros/', ['.jpg']))\n",
    "print 'loaded', len(arros), \"exemples d'arròs\"\n",
    "carn = list(list_all_files('../img_classifier/carn_2/', ['.jpg']))\n",
    "print 'loaded', len(carn), \"exemples de carn\"\n",
    "entrepans = list(list_all_files('../img_classifier/entrepans/', ['.jpg']))\n",
    "print 'loaded', len(entrepans), \"exemples d'entrepans\"\n",
    "pasta = list(list_all_files('../img_classifier/pasta/', ['.jpg']))\n",
    "print 'loaded', len(pasta), \"exemples de pasta\"\n",
    "pizza = list(list_all_files('../img_classifier/pizza/', ['.jpg']))\n",
    "print 'loaded', len(pizza), \"exemples de pizza\"\n",
    "sopa = list(list_all_files('../img_classifier/sopa/', ['.jpg']))\n",
    "print 'loaded', len(sopa), \"exemples de sopa\"\n",
    "sushi = list(list_all_files('../img_classifier/sushi_2/', ['.jpg']))\n",
    "print 'loaded', len(sushi), \"exemples de sushi\"\n",
    "'''carne = list(list_all_files('../japanese_food/carn/', ['.jpg']))\n",
    "print 'loaded', len(carne), \"exemples de carne\"'''\n",
    "'''tempura = list(list_all_files('../../notebooks/projecte/UBdatascience/images_classifier/Tempura/', ['.jpg']))\n",
    "print 'loaded', len(tempura), \"exemples de tempura\"'''\n",
    "verd = list(list_all_files('../img_classifier/verd/', ['.jpg']))\n",
    "print 'loaded', len(verd), \"exemples de verd\"\n",
    "examples = [(path, 0) for path in arros]+[(path, 1) for path in carn]+[(path, 2) for path in entrepans] \\\n",
    "+[(path, 3) for path in pasta]+[(path, 4) for path in pizza]+[(path, 5) for path in sopa]\\\n",
    "+[(path, 6) for path in sushi]+[(path, 7) for path in verd]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 3s, sys: 8.28 s, total: 4min 11s\n",
      "Wall time: 11min 44s\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from skimage.measure import block_reduce\n",
    "from skimage.io import imread\n",
    "from skimage import transform\n",
    "\n",
    "def examples_to_dataset(examples, block_size=2):\n",
    "    X = []\n",
    "    y = []\n",
    "    number = 0\n",
    "    sum_img_mean = 0\n",
    "    size = 112\n",
    "    for path, label in examples:        \n",
    "        try:\n",
    "            img = imread(path) #, as_grey=True\n",
    "            rows = img.shape[0]\n",
    "            cols = img.shape[1]\n",
    "            center = np.array([rows/2, cols/2])     \n",
    "\n",
    "            if (rows>cols):\n",
    "                crop_size = cols/2\n",
    "                img = img[center[0]-crop_size:center[0]+crop_size,:,:]\n",
    "            else : \n",
    "                crop_size = rows/2 \n",
    "                img = img[:,center[1]-crop_size:center[1]+crop_size,:]\n",
    "\n",
    "            img = transform.resize(img, (size,size))      \n",
    "            img = block_reduce(img, block_size=(2, 2, 1), func=np.mean)\n",
    "            img = img.reshape(56*56*3)\n",
    "            X.append(img)\n",
    "\n",
    "            if(label==0):\n",
    "                y.append((1,0,0,0,0,0,0,0))\n",
    "            elif(label==1):\n",
    "                y.append((0,1,0,0,0,0,0,0))\n",
    "            elif(label==2):\n",
    "                y.append((0,0,1,0,0,0,0,0))\n",
    "            elif(label==3):\n",
    "                y.append((0,0,0,1,0,0,0,0))\n",
    "            elif(label==4):\n",
    "                y.append((0,0,0,0,1,0,0,0))\n",
    "            elif(label==5):\n",
    "                y.append((0,0,0,0,0,1,0,0))\n",
    "            elif(label==6):\n",
    "                y.append((0,0,0,0,0,0,1,0))\n",
    "            else:\n",
    "                y.append((0,0,0,0,0,0,0,1))\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "    return np.asarray(X), np.asarray(y)\n",
    "\n",
    "%time X, Y = examples_to_dataset(examples)\n",
    "X = np.asarray(X,dtype=np.float32)\n",
    "Y = np.asarray(Y,dtype=np.int32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(53288, 9408)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Split data into train and test set\n",
    "from sklearn.cross_validation import train_test_split\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.05, random_state=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mean = np.mean(X_train, axis=1).mean()\n",
    "X_train = X_train - mean\n",
    "X_test = X_test - mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data = pd.DataFrame(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(y_test.shape[0]):\n",
    "    if np.array_equal(y_test[i],(1,0,0,0,0,0,0,0)):\n",
    "        labels.append(0)\n",
    "    if np.array_equal(y_test[i],(0,1,0,0,0,0,0,0)):\n",
    "        labels.append(1)\n",
    "    if np.array_equal(y_test[i],(0,0,1,0,0,0,0,0)):\n",
    "        labels.append(2)\n",
    "    if np.array_equal(y_test[i],(0,0,0,1,0,0,0,0)):\n",
    "        labels.append(3)\n",
    "    if np.array_equal(y_test[i],(0,0,0,0,1,0,0,0)):\n",
    "        labels.append(4)\n",
    "    if np.array_equal(y_test[i],(0,0,0,0,0,1,0,0)):\n",
    "        labels.append(5)\n",
    "    if np.array_equal(y_test[i],(0,0,0,0,0,0,1,0)):\n",
    "        labels.append(6)\n",
    "    if np.array_equal(y_test[i],(0,0,0,0,0,0,0,1)):\n",
    "        labels.append(7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data['labels']=labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arros = np.array(data[data['labels']==0].drop('labels',1))\n",
    "test_carn = np.array(data[data['labels']==1].drop('labels',1))\n",
    "test_entrepans = np.array(data[data['labels']==2].drop('labels',1))\n",
    "test_pasta = np.array(data[data['labels']==3].drop('labels',1))\n",
    "test_pizza = np.array(data[data['labels']==4].drop('labels',1))\n",
    "test_sopa = np.array(data[data['labels']==5].drop('labels',1))\n",
    "test_sushi = np.array(data[data['labels']==6].drop('labels',1))\n",
    "test_verd = np.array(data[data['labels']==7].drop('labels',1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_arros_y = np.array([(1,0,0,0,0,0,0,0) for i in range(test_arros.shape[0])])\n",
    "test_carn_y = np.array([(0,1,0,0,0,0,0,0) for i in range(test_carn.shape[0])])\n",
    "test_entrepans_y = np.array([(0,0,1,0,0,0,0,0) for i in range(test_entrepans.shape[0])])\n",
    "test_pasta_y = np.array([(0,0,0,1,0,0,0,0) for i in range(test_pasta.shape[0])])\n",
    "test_pizza_y = np.array([(0,0,0,0,1,0,0,0) for i in range(test_pizza.shape[0])])\n",
    "test_sopa_y = np.array([(0,0,0,0,0,1,0,0) for i in range(test_sopa.shape[0])])\n",
    "test_sushi_y = np.array([(0,0,0,0,0,0,1,0) for i in range(test_sushi.shape[0])])\n",
    "test_verd_y = np.array([(0,0,0,0,0,0,0,1) for i in range(test_verd.shape[0])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50623, 9408)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2665, 9408)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#show some pics if in gray scale\n",
    "#plt.imshow(255*make_mosaic(X[:len(pasta)], 8),cmap='gray') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.0001\n",
    "training_iters = 52000\n",
    "batch_size = 10\n",
    "display_step = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "n_input = 9408 # MNIST data input (img shape: 28*28)\n",
    "n_classes = 8 # MNIST total classes (0-9 digits)\n",
    "dropout = 0.75 # Dropout, probability to keep units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tf Graph input\n",
    "x = tf.placeholder(tf.float32, [None, n_input])\n",
    "y = tf.placeholder(tf.float32, [None, n_classes])\n",
    "keep_prob = tf.placeholder(tf.float32) #dropout (keep probability)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create AlexNet model\n",
    "def conv2d(name, l_input, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(l_input, w, strides=[1, 1, 1, 1], \n",
    "                                                  padding='SAME'),b), name=name)\n",
    "\n",
    "def max_pool(name, l_input, k):\n",
    "    return tf.nn.max_pool(l_input, ksize=[1, k, k, 1], strides=[1, k, k, 1], \n",
    "                          padding='SAME', name=name)\n",
    "\n",
    "def norm(name, l_input, lsize=4):\n",
    "    return tf.nn.lrn(l_input, lsize, bias=1.0, alpha=0.001 / 9.0, beta=0.75, name=name)\n",
    "\n",
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture\n",
    "    _X = tf.reshape(_X, shape=[-1, 56, 56, 3])\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv1 = conv2d('conv1', _X, _weights['wc1'], _biases['bc1'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool1 = max_pool('pool1', conv1, k=2)\n",
    "    # Apply Normalization\n",
    "    norm1 = norm('norm1', pool1, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm1 = tf.nn.dropout(norm1, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv2 = conv2d('conv2', norm1, _weights['wc2'], _biases['bc2'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool2 = max_pool('pool2', conv2, k=2)\n",
    "    # Apply Normalization\n",
    "    norm2 = norm('norm2', pool2, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm2 = tf.nn.dropout(norm2, _dropout)\n",
    "\n",
    "    # Convolution Layer\n",
    "    conv3 = conv2d('conv3', norm2, _weights['wc3'], _biases['bc3'])\n",
    "    # Max Pooling (down-sampling)\n",
    "    pool3 = max_pool('pool3', conv3, k=2)\n",
    "    # Apply Normalization\n",
    "    norm3 = norm('norm3', pool3, lsize=4)\n",
    "    # Apply Dropout\n",
    "    norm3 = tf.nn.dropout(norm3, _dropout)\n",
    "\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    # Relu activation\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')\n",
    "    \n",
    "    # Relu activation\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') \n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([5, 5, 3, 96],stddev=0.01)),\n",
    "    'wc2': tf.Variable(tf.random_normal([5, 5, 96, 192],stddev=0.01)),\n",
    "    'wc3': tf.Variable(tf.random_normal([5, 5, 192, 192],stddev=0.01)),\n",
    "    'wd1': tf.Variable(tf.random_normal([7*7*192, 1024],stddev=0.01)),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024],stddev=0.01)),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 8],stddev=0.01))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([96],stddev=0.01)),\n",
    "    'bc2': tf.Variable(tf.random_normal([192],stddev=0.01)),\n",
    "    'bc3': tf.Variable(tf.random_normal([192],stddev=0.01)),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024],stddev=0.01)),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024],stddev=0.01)),\n",
    "    'out': tf.Variable(tf.random_normal([n_classes]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Construct model\n",
    "pred = alex_net(x, weights, biases, keep_prob)\n",
    "\n",
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)\n",
    "\n",
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n",
    "\n",
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 200, Minibatch Loss= 2.053283, Training Accuracy= 0.10000\n",
      "Iter 400, Minibatch Loss= 2.028992, Training Accuracy= 0.20000\n",
      "Iter 600, Minibatch Loss= 1.894882, Training Accuracy= 0.20000\n",
      "Iter 800, Minibatch Loss= 2.106529, Training Accuracy= 0.10000\n",
      "Iter 1000, Minibatch Loss= 1.755827, Training Accuracy= 0.40000\n",
      "Iter 1200, Minibatch Loss= 2.057027, Training Accuracy= 0.40000\n",
      "Iter 1400, Minibatch Loss= 1.944765, Training Accuracy= 0.30000\n",
      "Iter 1600, Minibatch Loss= 1.991502, Training Accuracy= 0.10000\n",
      "Iter 1800, Minibatch Loss= 1.818081, Training Accuracy= 0.30000\n",
      "Iter 2000, Minibatch Loss= 2.087934, Training Accuracy= 0.20000\n",
      "Iter 2200, Minibatch Loss= 1.716215, Training Accuracy= 0.30000\n",
      "Iter 2400, Minibatch Loss= 2.221374, Training Accuracy= 0.10000\n",
      "Iter 2600, Minibatch Loss= 1.671345, Training Accuracy= 0.60000\n",
      "Iter 2800, Minibatch Loss= 1.882456, Training Accuracy= 0.40000\n",
      "Iter 3000, Minibatch Loss= 1.857820, Training Accuracy= 0.20000\n",
      "Iter 3200, Minibatch Loss= 1.811737, Training Accuracy= 0.50000\n",
      "Iter 3400, Minibatch Loss= 1.760718, Training Accuracy= 0.20000\n",
      "Iter 3600, Minibatch Loss= 1.926748, Training Accuracy= 0.40000\n",
      "Iter 3800, Minibatch Loss= 2.322826, Training Accuracy= 0.10000\n",
      "Iter 4000, Minibatch Loss= 1.637139, Training Accuracy= 0.50000\n",
      "Iter 4200, Minibatch Loss= 1.773343, Training Accuracy= 0.40000\n",
      "Iter 4400, Minibatch Loss= 1.977032, Training Accuracy= 0.30000\n",
      "Iter 4600, Minibatch Loss= 2.241043, Training Accuracy= 0.00000\n",
      "Iter 4800, Minibatch Loss= 2.159024, Training Accuracy= 0.20000\n",
      "Iter 5000, Minibatch Loss= 1.624261, Training Accuracy= 0.50000\n",
      "Iter 5200, Minibatch Loss= 2.073040, Training Accuracy= 0.00000\n",
      "Iter 5400, Minibatch Loss= 2.062991, Training Accuracy= 0.30000\n",
      "Iter 5600, Minibatch Loss= 1.642782, Training Accuracy= 0.40000\n",
      "Iter 5800, Minibatch Loss= 1.971910, Training Accuracy= 0.40000\n",
      "Iter 6000, Minibatch Loss= 1.802528, Training Accuracy= 0.50000\n",
      "Iter 6200, Minibatch Loss= 2.262686, Training Accuracy= 0.20000\n",
      "Iter 6400, Minibatch Loss= 1.890711, Training Accuracy= 0.20000\n",
      "Iter 6600, Minibatch Loss= 2.108445, Training Accuracy= 0.30000\n",
      "Iter 6800, Minibatch Loss= 2.021523, Training Accuracy= 0.20000\n",
      "Iter 7000, Minibatch Loss= 1.689249, Training Accuracy= 0.40000\n",
      "Iter 7200, Minibatch Loss= 1.881057, Training Accuracy= 0.30000\n",
      "Iter 7400, Minibatch Loss= 1.973374, Training Accuracy= 0.20000\n",
      "Iter 7600, Minibatch Loss= 1.619723, Training Accuracy= 0.30000\n",
      "Iter 7800, Minibatch Loss= 1.880970, Training Accuracy= 0.30000\n",
      "Iter 8000, Minibatch Loss= 1.921989, Training Accuracy= 0.40000\n",
      "Iter 8200, Minibatch Loss= 1.879056, Training Accuracy= 0.10000\n",
      "Iter 8400, Minibatch Loss= 1.794670, Training Accuracy= 0.30000\n",
      "Iter 8600, Minibatch Loss= 1.810518, Training Accuracy= 0.30000\n",
      "Iter 8800, Minibatch Loss= 1.664729, Training Accuracy= 0.30000\n",
      "Iter 9000, Minibatch Loss= 2.027985, Training Accuracy= 0.10000\n",
      "Iter 9200, Minibatch Loss= 2.221314, Training Accuracy= 0.10000\n",
      "Iter 9400, Minibatch Loss= 1.580725, Training Accuracy= 0.50000\n",
      "Iter 9600, Minibatch Loss= 2.346194, Training Accuracy= 0.40000\n",
      "Iter 9800, Minibatch Loss= 2.008556, Training Accuracy= 0.20000\n",
      "Iter 10000, Minibatch Loss= 1.705166, Training Accuracy= 0.40000\n",
      "Iter 10200, Minibatch Loss= 2.142262, Training Accuracy= 0.10000\n",
      "Iter 10400, Minibatch Loss= 1.876220, Training Accuracy= 0.30000\n",
      "Iter 10600, Minibatch Loss= 1.815983, Training Accuracy= 0.30000\n",
      "Iter 10800, Minibatch Loss= 1.754390, Training Accuracy= 0.40000\n",
      "Iter 11000, Minibatch Loss= 1.756630, Training Accuracy= 0.40000\n",
      "Iter 11200, Minibatch Loss= 1.471085, Training Accuracy= 0.30000\n",
      "Iter 11400, Minibatch Loss= 1.917125, Training Accuracy= 0.50000\n",
      "Iter 11600, Minibatch Loss= 2.017189, Training Accuracy= 0.20000\n",
      "Iter 11800, Minibatch Loss= 2.071905, Training Accuracy= 0.20000\n",
      "Iter 12000, Minibatch Loss= 1.877296, Training Accuracy= 0.20000\n",
      "Iter 12200, Minibatch Loss= 1.644276, Training Accuracy= 0.40000\n",
      "Iter 12400, Minibatch Loss= 1.992319, Training Accuracy= 0.30000\n",
      "Iter 12600, Minibatch Loss= 1.893729, Training Accuracy= 0.20000\n",
      "Iter 12800, Minibatch Loss= 2.421455, Training Accuracy= 0.10000\n",
      "Iter 13000, Minibatch Loss= 1.693297, Training Accuracy= 0.30000\n",
      "Iter 13200, Minibatch Loss= 1.633185, Training Accuracy= 0.40000\n",
      "Iter 13400, Minibatch Loss= 1.791428, Training Accuracy= 0.30000\n",
      "Iter 13600, Minibatch Loss= 1.851534, Training Accuracy= 0.40000\n",
      "Iter 13800, Minibatch Loss= 1.693851, Training Accuracy= 0.20000\n",
      "Iter 14000, Minibatch Loss= 1.874104, Training Accuracy= 0.30000\n",
      "Iter 14200, Minibatch Loss= 1.874236, Training Accuracy= 0.40000\n",
      "Iter 14400, Minibatch Loss= 1.533248, Training Accuracy= 0.60000\n",
      "Iter 14600, Minibatch Loss= 2.109893, Training Accuracy= 0.20000\n",
      "Iter 14800, Minibatch Loss= 1.864876, Training Accuracy= 0.40000\n",
      "Iter 15000, Minibatch Loss= 1.416676, Training Accuracy= 0.60000\n",
      "Iter 15200, Minibatch Loss= 1.718506, Training Accuracy= 0.40000\n",
      "Iter 15400, Minibatch Loss= 1.848277, Training Accuracy= 0.10000\n",
      "Iter 15600, Minibatch Loss= 1.485663, Training Accuracy= 0.50000\n",
      "Iter 15800, Minibatch Loss= 1.531094, Training Accuracy= 0.40000\n",
      "Iter 16000, Minibatch Loss= 1.678022, Training Accuracy= 0.40000\n",
      "Iter 16200, Minibatch Loss= 1.741592, Training Accuracy= 0.40000\n",
      "Iter 16400, Minibatch Loss= 1.776337, Training Accuracy= 0.20000\n",
      "Iter 16600, Minibatch Loss= 1.836498, Training Accuracy= 0.30000\n",
      "Iter 16800, Minibatch Loss= 1.650848, Training Accuracy= 0.50000\n",
      "Iter 17000, Minibatch Loss= 1.979490, Training Accuracy= 0.20000\n",
      "Iter 17200, Minibatch Loss= 1.941645, Training Accuracy= 0.30000\n",
      "Iter 17400, Minibatch Loss= 1.541951, Training Accuracy= 0.30000\n",
      "Iter 17600, Minibatch Loss= 1.585965, Training Accuracy= 0.50000\n",
      "Iter 17800, Minibatch Loss= 1.706356, Training Accuracy= 0.20000\n",
      "Iter 18000, Minibatch Loss= 1.877173, Training Accuracy= 0.30000\n",
      "Iter 18200, Minibatch Loss= 1.549565, Training Accuracy= 0.40000\n",
      "Iter 18400, Minibatch Loss= 1.534457, Training Accuracy= 0.50000\n",
      "Iter 18600, Minibatch Loss= 1.298445, Training Accuracy= 0.50000\n",
      "Iter 18800, Minibatch Loss= 1.620541, Training Accuracy= 0.30000\n",
      "Iter 19000, Minibatch Loss= 2.150745, Training Accuracy= 0.20000\n",
      "Iter 19200, Minibatch Loss= 1.791235, Training Accuracy= 0.30000\n",
      "Iter 19400, Minibatch Loss= 1.401811, Training Accuracy= 0.60000\n",
      "Iter 19600, Minibatch Loss= 1.465863, Training Accuracy= 0.40000\n",
      "Iter 19800, Minibatch Loss= 1.561810, Training Accuracy= 0.50000\n",
      "Iter 20000, Minibatch Loss= 1.031143, Training Accuracy= 1.00000\n",
      "Iter 20200, Minibatch Loss= 1.484311, Training Accuracy= 0.30000\n",
      "Iter 20400, Minibatch Loss= 1.961085, Training Accuracy= 0.30000\n",
      "Iter 20600, Minibatch Loss= 1.635577, Training Accuracy= 0.70000\n",
      "Iter 20800, Minibatch Loss= 1.809774, Training Accuracy= 0.50000\n",
      "Iter 21000, Minibatch Loss= 1.711246, Training Accuracy= 0.30000\n",
      "Iter 21200, Minibatch Loss= 1.537538, Training Accuracy= 0.60000\n",
      "Iter 21400, Minibatch Loss= 1.520457, Training Accuracy= 0.40000\n",
      "Iter 21600, Minibatch Loss= 1.791244, Training Accuracy= 0.40000\n",
      "Iter 21800, Minibatch Loss= 1.391437, Training Accuracy= 0.50000\n",
      "Iter 22000, Minibatch Loss= 1.591223, Training Accuracy= 0.40000\n",
      "Iter 22200, Minibatch Loss= 1.439015, Training Accuracy= 0.50000\n",
      "Iter 22400, Minibatch Loss= 2.171179, Training Accuracy= 0.10000\n",
      "Iter 22600, Minibatch Loss= 1.709288, Training Accuracy= 0.30000\n",
      "Iter 22800, Minibatch Loss= 1.625736, Training Accuracy= 0.50000\n",
      "Iter 23000, Minibatch Loss= 1.848148, Training Accuracy= 0.30000\n",
      "Iter 23200, Minibatch Loss= 1.683102, Training Accuracy= 0.30000\n",
      "Iter 23400, Minibatch Loss= 1.769735, Training Accuracy= 0.10000\n",
      "Iter 23600, Minibatch Loss= 2.200122, Training Accuracy= 0.20000\n",
      "Iter 23800, Minibatch Loss= 1.360904, Training Accuracy= 0.70000\n",
      "Iter 24000, Minibatch Loss= 1.467610, Training Accuracy= 0.40000\n",
      "Iter 24200, Minibatch Loss= 1.321235, Training Accuracy= 0.50000\n",
      "Iter 24400, Minibatch Loss= 1.922642, Training Accuracy= 0.30000\n",
      "Iter 24600, Minibatch Loss= 1.343375, Training Accuracy= 0.50000\n",
      "Iter 24800, Minibatch Loss= 1.485857, Training Accuracy= 0.50000\n",
      "Iter 25000, Minibatch Loss= 1.678778, Training Accuracy= 0.50000\n",
      "Iter 25200, Minibatch Loss= 1.486125, Training Accuracy= 0.30000\n",
      "Iter 25400, Minibatch Loss= 1.606143, Training Accuracy= 0.40000\n",
      "Iter 25600, Minibatch Loss= 1.644894, Training Accuracy= 0.40000\n",
      "Iter 25800, Minibatch Loss= 1.536480, Training Accuracy= 0.50000\n",
      "Iter 26000, Minibatch Loss= 1.495471, Training Accuracy= 0.60000\n",
      "Iter 26200, Minibatch Loss= 1.630890, Training Accuracy= 0.30000\n",
      "Iter 26400, Minibatch Loss= 1.576858, Training Accuracy= 0.30000\n",
      "Iter 26600, Minibatch Loss= 1.702634, Training Accuracy= 0.20000\n",
      "Iter 26800, Minibatch Loss= 2.030901, Training Accuracy= 0.20000\n",
      "Iter 27000, Minibatch Loss= 1.648738, Training Accuracy= 0.30000\n",
      "Iter 27200, Minibatch Loss= 1.616739, Training Accuracy= 0.30000\n",
      "Iter 27400, Minibatch Loss= 1.886948, Training Accuracy= 0.20000\n",
      "Iter 27600, Minibatch Loss= 1.765189, Training Accuracy= 0.40000\n",
      "Iter 27800, Minibatch Loss= 1.701181, Training Accuracy= 0.50000\n",
      "Iter 28000, Minibatch Loss= 2.301234, Training Accuracy= 0.30000\n",
      "Iter 28200, Minibatch Loss= 1.375866, Training Accuracy= 0.40000\n",
      "Iter 28400, Minibatch Loss= 1.609881, Training Accuracy= 0.40000\n",
      "Iter 28600, Minibatch Loss= 1.499351, Training Accuracy= 0.40000\n",
      "Iter 28800, Minibatch Loss= 1.672493, Training Accuracy= 0.50000\n",
      "Iter 29000, Minibatch Loss= 1.409762, Training Accuracy= 0.50000\n",
      "Iter 29200, Minibatch Loss= 1.500828, Training Accuracy= 0.40000\n",
      "Iter 29400, Minibatch Loss= 1.673370, Training Accuracy= 0.40000\n",
      "Iter 29600, Minibatch Loss= 1.466648, Training Accuracy= 0.40000\n",
      "Iter 29800, Minibatch Loss= 1.577789, Training Accuracy= 0.30000\n",
      "Iter 30000, Minibatch Loss= 1.243057, Training Accuracy= 0.60000\n",
      "Iter 30200, Minibatch Loss= 1.553614, Training Accuracy= 0.50000\n",
      "Iter 30400, Minibatch Loss= 1.505934, Training Accuracy= 0.60000\n",
      "Iter 30600, Minibatch Loss= 1.437852, Training Accuracy= 0.50000\n",
      "Iter 30800, Minibatch Loss= 1.429482, Training Accuracy= 0.60000\n",
      "Iter 31000, Minibatch Loss= 2.036170, Training Accuracy= 0.20000\n",
      "Iter 31200, Minibatch Loss= 1.381345, Training Accuracy= 0.40000\n",
      "Iter 31400, Minibatch Loss= 1.288013, Training Accuracy= 0.60000\n",
      "Iter 31600, Minibatch Loss= 1.096099, Training Accuracy= 0.70000\n",
      "Iter 31800, Minibatch Loss= 1.557507, Training Accuracy= 0.40000\n",
      "Iter 32000, Minibatch Loss= 1.252208, Training Accuracy= 0.60000\n",
      "Iter 32200, Minibatch Loss= 1.618777, Training Accuracy= 0.50000\n",
      "Iter 32400, Minibatch Loss= 1.522113, Training Accuracy= 0.50000\n",
      "Iter 32600, Minibatch Loss= 1.922913, Training Accuracy= 0.40000\n",
      "Iter 32800, Minibatch Loss= 1.303761, Training Accuracy= 0.50000\n",
      "Iter 33000, Minibatch Loss= 1.773435, Training Accuracy= 0.40000\n",
      "Iter 33200, Minibatch Loss= 1.873185, Training Accuracy= 0.20000\n",
      "Iter 33400, Minibatch Loss= 1.800112, Training Accuracy= 0.40000\n",
      "Iter 33600, Minibatch Loss= 1.905924, Training Accuracy= 0.10000\n",
      "Iter 33800, Minibatch Loss= 1.275434, Training Accuracy= 0.60000\n",
      "Iter 34000, Minibatch Loss= 1.639396, Training Accuracy= 0.20000\n",
      "Iter 34200, Minibatch Loss= 1.386318, Training Accuracy= 0.40000\n",
      "Iter 34400, Minibatch Loss= 1.935745, Training Accuracy= 0.30000\n",
      "Iter 34600, Minibatch Loss= 1.175889, Training Accuracy= 0.50000\n",
      "Iter 34800, Minibatch Loss= 1.605773, Training Accuracy= 0.30000\n",
      "Iter 35000, Minibatch Loss= 1.552337, Training Accuracy= 0.60000\n",
      "Iter 35200, Minibatch Loss= 1.470322, Training Accuracy= 0.50000\n",
      "Iter 35400, Minibatch Loss= 1.146355, Training Accuracy= 0.60000\n",
      "Iter 35600, Minibatch Loss= 1.427766, Training Accuracy= 0.50000\n",
      "Iter 35800, Minibatch Loss= 1.454241, Training Accuracy= 0.20000\n",
      "Iter 36000, Minibatch Loss= 1.418962, Training Accuracy= 0.60000\n",
      "Iter 36200, Minibatch Loss= 1.353657, Training Accuracy= 0.50000\n",
      "Iter 36400, Minibatch Loss= 1.288167, Training Accuracy= 0.40000\n",
      "Iter 36600, Minibatch Loss= 1.569152, Training Accuracy= 0.30000\n",
      "Iter 36800, Minibatch Loss= 1.140106, Training Accuracy= 0.50000\n",
      "Iter 37000, Minibatch Loss= 0.946502, Training Accuracy= 0.70000\n",
      "Iter 37200, Minibatch Loss= 1.209795, Training Accuracy= 0.60000\n",
      "Iter 37400, Minibatch Loss= 1.847446, Training Accuracy= 0.30000\n",
      "Iter 37600, Minibatch Loss= 1.122925, Training Accuracy= 0.50000\n",
      "Iter 37800, Minibatch Loss= 1.124888, Training Accuracy= 0.60000\n",
      "Iter 38000, Minibatch Loss= 1.422636, Training Accuracy= 0.50000\n",
      "Iter 38200, Minibatch Loss= 1.219970, Training Accuracy= 0.70000\n",
      "Iter 38400, Minibatch Loss= 2.084353, Training Accuracy= 0.40000\n",
      "Iter 38600, Minibatch Loss= 1.674896, Training Accuracy= 0.20000\n",
      "Iter 38800, Minibatch Loss= 1.123881, Training Accuracy= 0.70000\n",
      "Iter 39000, Minibatch Loss= 1.417316, Training Accuracy= 0.50000\n",
      "Iter 39200, Minibatch Loss= 1.183219, Training Accuracy= 0.50000\n",
      "Iter 39400, Minibatch Loss= 1.633347, Training Accuracy= 0.30000\n",
      "Iter 39600, Minibatch Loss= 0.883623, Training Accuracy= 0.70000\n",
      "Iter 39800, Minibatch Loss= 1.195761, Training Accuracy= 0.60000\n",
      "Iter 40000, Minibatch Loss= 0.976598, Training Accuracy= 0.80000\n",
      "Iter 40200, Minibatch Loss= 1.656811, Training Accuracy= 0.40000\n",
      "Iter 40400, Minibatch Loss= 1.678733, Training Accuracy= 0.30000\n",
      "Iter 40600, Minibatch Loss= 1.098479, Training Accuracy= 0.50000\n",
      "Iter 40800, Minibatch Loss= 1.257977, Training Accuracy= 0.40000\n",
      "Iter 41000, Minibatch Loss= 1.298972, Training Accuracy= 0.50000\n",
      "Iter 41200, Minibatch Loss= 1.432075, Training Accuracy= 0.60000\n",
      "Iter 41400, Minibatch Loss= 1.334095, Training Accuracy= 0.50000\n",
      "Iter 41600, Minibatch Loss= 1.534449, Training Accuracy= 0.40000\n",
      "Iter 41800, Minibatch Loss= 1.492124, Training Accuracy= 0.40000\n",
      "Iter 42000, Minibatch Loss= 1.157623, Training Accuracy= 0.40000\n",
      "Iter 42200, Minibatch Loss= 2.084258, Training Accuracy= 0.20000\n",
      "Iter 42400, Minibatch Loss= 1.028790, Training Accuracy= 0.80000\n",
      "Iter 42600, Minibatch Loss= 1.489796, Training Accuracy= 0.20000\n",
      "Iter 42800, Minibatch Loss= 1.240245, Training Accuracy= 0.50000\n",
      "Iter 43000, Minibatch Loss= 1.389633, Training Accuracy= 0.50000\n",
      "Iter 43200, Minibatch Loss= 1.680522, Training Accuracy= 0.40000\n",
      "Iter 43400, Minibatch Loss= 1.700457, Training Accuracy= 0.20000\n",
      "Iter 43600, Minibatch Loss= 1.717454, Training Accuracy= 0.30000\n",
      "Iter 43800, Minibatch Loss= 1.539664, Training Accuracy= 0.40000\n",
      "Iter 44000, Minibatch Loss= 1.145106, Training Accuracy= 0.70000\n",
      "Iter 44200, Minibatch Loss= 1.340733, Training Accuracy= 0.60000\n",
      "Iter 44400, Minibatch Loss= 1.033313, Training Accuracy= 0.50000\n",
      "Iter 44600, Minibatch Loss= 1.606465, Training Accuracy= 0.50000\n",
      "Iter 44800, Minibatch Loss= 1.008957, Training Accuracy= 0.70000\n",
      "Iter 45000, Minibatch Loss= 2.479550, Training Accuracy= 0.10000\n",
      "Iter 45200, Minibatch Loss= 1.784543, Training Accuracy= 0.30000\n",
      "Iter 45400, Minibatch Loss= 1.225531, Training Accuracy= 0.60000\n",
      "Iter 45600, Minibatch Loss= 1.693010, Training Accuracy= 0.30000\n",
      "Iter 45800, Minibatch Loss= 1.504051, Training Accuracy= 0.40000\n",
      "Iter 46000, Minibatch Loss= 1.031054, Training Accuracy= 0.60000\n",
      "Iter 46200, Minibatch Loss= 1.143290, Training Accuracy= 0.70000\n",
      "Iter 46400, Minibatch Loss= 1.851078, Training Accuracy= 0.30000\n",
      "Iter 46600, Minibatch Loss= 1.331871, Training Accuracy= 0.40000\n",
      "Iter 46800, Minibatch Loss= 1.438138, Training Accuracy= 0.50000\n",
      "Iter 47000, Minibatch Loss= 1.505077, Training Accuracy= 0.30000\n",
      "Iter 47200, Minibatch Loss= 1.157514, Training Accuracy= 0.50000\n",
      "Iter 47400, Minibatch Loss= 1.688135, Training Accuracy= 0.30000\n",
      "Iter 47600, Minibatch Loss= 1.006253, Training Accuracy= 0.70000\n",
      "Iter 47800, Minibatch Loss= 1.627538, Training Accuracy= 0.50000\n",
      "Iter 48000, Minibatch Loss= 1.615697, Training Accuracy= 0.40000\n",
      "Iter 48200, Minibatch Loss= 1.454117, Training Accuracy= 0.40000\n",
      "Iter 48400, Minibatch Loss= 1.636797, Training Accuracy= 0.30000\n",
      "Iter 48600, Minibatch Loss= 1.493840, Training Accuracy= 0.50000\n",
      "Iter 48800, Minibatch Loss= 1.350503, Training Accuracy= 0.60000\n",
      "Iter 49000, Minibatch Loss= 0.824898, Training Accuracy= 0.90000\n",
      "Iter 49200, Minibatch Loss= 0.835458, Training Accuracy= 0.80000\n",
      "Iter 49400, Minibatch Loss= 1.530036, Training Accuracy= 0.50000\n",
      "Iter 49600, Minibatch Loss= 1.730805, Training Accuracy= 0.50000\n",
      "Iter 49800, Minibatch Loss= 1.410450, Training Accuracy= 0.40000\n",
      "Optimization Finished!\n",
      "Testing Accuracy: 0.5197\n",
      "Testing Accuracy for arros: 0.344928\n",
      "Testing Accuracy for carn: 0.486486\n",
      "Testing Accuracy for entrepans: 0.309963\n",
      "Testing Accuracy for pasta: 0.509375\n",
      "Testing Accuracy for pizza: 0.425806\n",
      "Testing Accuracy for sopa: 0.629857\n",
      "Testing Accuracy for sushi: 0.628378\n",
      "Testing Accuracy for verd: 0.651316\n"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = X_train[(batch_size*(step-1)):batch_size*(step)],y_train[(batch_size*(step-1)):batch_size*(step)]\n",
    "        # Run optimization op (backprop)\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys,\n",
    "                                       keep_prob: dropout})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch loss and accuracy\n",
    "            loss, acc = sess.run([cost, accuracy], feed_dict={x: batch_xs,\n",
    "                                                              y: batch_ys,\n",
    "                                                              keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" + \\\n",
    "                  \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \\\n",
    "                  \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "\n",
    "    # Calculate accuracy for test images\n",
    "    print \"Testing Accuracy:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: X_test,\n",
    "                                      y: y_test,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for arros:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_arros,\n",
    "                                      y: test_arros_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for carn:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_carn,\n",
    "                                      y: test_carn_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for entrepans:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_entrepans,\n",
    "                                      y: test_entrepans_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for pasta:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_pasta,\n",
    "                                      y: test_pasta_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for pizza:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_pizza,\n",
    "                                      y: test_pizza_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for sopa:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_sopa,\n",
    "                                      y: test_sopa_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for sushi:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_sushi,\n",
    "                                      y: test_sushi_y,\n",
    "                                      keep_prob: 1.})\n",
    "    print \"Testing Accuracy for verd:\", \\\n",
    "        sess.run(accuracy, feed_dict={x: test_verd,\n",
    "                                      y: test_verd_y,\n",
    "                                      keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classificador de tags (II)\n",
    "\n",
    "Entrenem un Multilabel Naive Bayes Classifier amb els tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "import pickle\n",
    "import math\n",
    "from random import shuffle\n",
    "import urllib2\n",
    "import re\n",
    "from urllib2 import URLError\n",
    "import os\n",
    "import time\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load and save lists and dictionaries with pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(''+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)\n",
    "    \n",
    "# normalize with L2 norm\n",
    "def l2_normalizer(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return [(el / math.sqrt(denom)) for el in vec]\n",
    "\n",
    "# L2 norm of a vector\n",
    "def l2_norm(vec):\n",
    "    denom = np.sum([el**2 for el in vec])\n",
    "    return math.sqrt(denom)\n",
    "\n",
    "def img_counter_per_rest_types(img_list):\n",
    "    n = len(set([item[1] for item in img_list]))\n",
    "    counters = [0]*n\n",
    "    for item in img_list:\n",
    "        for caca in range(8):\n",
    "            if item[1] == caca:\n",
    "                counters[caca] = counters[caca] + 1\n",
    "    return counters\n",
    "\n",
    "def listHashtags(string): \n",
    "    hashtaglist = []\n",
    "    while string.find('#',0) != -1:\n",
    "        m = re.search('(?<=#)\\w+', string)\n",
    "        string = string.replace('#'+m.group(0),'')\n",
    "        hashtaglist.append(m.group(0))\n",
    "    hashtaglist = [item.lower() for item in hashtaglist]\n",
    "    return hashtaglist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load rest_list (list of dicts, each a rest, with all the info (name, loc, code, images, ...))\n",
    "rest_list = load_obj('rest_list')\n",
    "# we just need tags and rest type (especialidad)\n",
    "img_list = []\n",
    "for rest in rest_list:\n",
    "    try:\n",
    "        for img in rest['IMAGES']:\n",
    "            if len(img['tags']) != 0:\n",
    "                img_list.append([img['tags'],rest['ESPECIALIDAD']])\n",
    "    except KeyError:\n",
    "        None\n",
    "# cocina casera -> 0, tapas -> 1, paellas y arroces -> 2, rapida -> 3, italiana -> 4, asiatica -> 5, carnes -> 6\n",
    "# dietetica -> 7\n",
    "rest_types = ['Cocina casera','Tapas, pintxos y platillos','Paellas y arroces','Cocina rapida']\n",
    "rest_types = rest_types + ['Cocina italiana','Cocina asiatica','Carnes a la brasa']\n",
    "rest_types = rest_types + ['Cocina dietetica, naturista, vegetariana y biologica']\n",
    "for img in img_list:\n",
    "    img[1] = rest_types.index(img[1])\n",
    "# every tag in lowercase\n",
    "for i in range(len(img_list)):\n",
    "    img_list[i] = [[item.lower() for item in img_list[i][0]] ,img_list[i][1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we don't want the entire data set, and since it is 'ordered' by restaurant, we shuffle it.\n",
    "shuffle(img_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's use pandas for taking just max_n_of_images pics (maximum) per rest type\n",
    "max_n_of_images = 9000\n",
    "img_list_df = pd.DataFrame(img_list)\n",
    "red_img_list = []\n",
    "for rest_type in range(8):\n",
    "    img_list_df_label = img_list_df[img_list_df[1] == rest_type][:max_n_of_images]\n",
    "    red_img_list = red_img_list + img_list_df_label.values.tolist()\n",
    "shuffle(red_img_list) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of the entire image set per rest_type (0-7): [2311, 8955, 684, 425, 1168, 1371, 1398, 585]\n",
      "Size of the train set per rest_type (0-7): [2080, 8060, 616, 383, 1052, 1234, 1259, 527]\n",
      "Size of the test set per rest_type (0-7): [231, 895, 68, 42, 116, 137, 139, 58]\n"
     ]
    }
   ],
   "source": [
    "# let us divide in train and test. red_rest_list will be the train set and img_list_test will be the test set.\n",
    "print 'Size of the entire image set per rest_type (0-7): ' + str(img_counter_per_rest_types(red_img_list))\n",
    "img_count_per_type = img_counter_per_rest_types(red_img_list)\n",
    "img_list_test = []\n",
    "n_images_test = [int(caca*0.1) for caca in img_count_per_type]\n",
    "counter = [0]*len(img_count_per_type)\n",
    "for img in red_img_list:\n",
    "    if counter[img[1]] < n_images_test[img[1]]:\n",
    "        img_list_test.append(img)\n",
    "        red_img_list.remove(img)\n",
    "        counter[img[1]] = counter[img[1]] + 1\n",
    "print 'Size of the train set per rest_type (0-7): ' + str(img_counter_per_rest_types(red_img_list))\n",
    "print 'Size of the test set per rest_type (0-7): ' + str(img_counter_per_rest_types(img_list_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of the dictionary: 23159 different tags\n"
     ]
    }
   ],
   "source": [
    "# construct the dictionary (every fucking word appearing as a tag in the train image set)\n",
    "dic = []\n",
    "for img in red_img_list:\n",
    "        for tag in img[0]:\n",
    "                dic.append(str(tag).lower())\n",
    "dic = list(set(dic))\n",
    "print 'Length of the dictionary: ' + str(len(dic)) + ' different tags'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# construct the frequency set: we associate to each image a list of length len(dic)\n",
    "freq_set = []\n",
    "for img in red_img_list:\n",
    "    zeroes = [0]*len(dic)\n",
    "    for tag in img[0]:\n",
    "        zeroes[dic.index(str(tag))] = 1\n",
    "    freq_set.append([zeroes,img[1]])\n",
    "\n",
    "test_freq_set = []\n",
    "for img in img_list_test:\n",
    "    zeroes = [0]*len(dic)\n",
    "    for tag in img[0]:\n",
    "        try:\n",
    "            zeroes[dic.index(str(tag))] = 1\n",
    "        except ValueError:\n",
    "            None\n",
    "    test_freq_set.append([zeroes,img[1]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# construct inverse document frequency matrix\n",
    "doclist = [item[0] for item in freq_set]\n",
    "n_samples = len(doclist)\n",
    "# how many times does a hashtag appear in all the samples?\n",
    "doclist_traspose = list(zip(*doclist))\n",
    "numContaining = [sum(col) for col in doclist_traspose]\n",
    "# finally compute idf vector -> idf matrix\n",
    "my_idf_vector = [np.log(n_samples / (float(caca))) for caca in numContaining]\n",
    "my_idf_matrix = np.zeros((len(my_idf_vector), len(my_idf_vector)))\n",
    "np.fill_diagonal(my_idf_matrix, my_idf_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "1%\n",
      "1%\n",
      "2%\n",
      "3%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "23%\n",
      "24%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "28%\n",
      "29%\n",
      "30%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "46%\n",
      "47%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "55%\n",
      "56%\n",
      "57%\n",
      "57%\n",
      "58%\n",
      "59%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "61%\n",
      "62%\n",
      "63%\n",
      "63%\n",
      "64%\n",
      "65%\n",
      "65%\n",
      "66%\n",
      "67%\n",
      "67%\n",
      "68%\n",
      "69%\n",
      "69%\n",
      "70%\n",
      "71%\n",
      "71%\n",
      "72%\n",
      "72%\n",
      "73%\n",
      "74%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "94%\n",
      "95%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "97%\n",
      "98%\n",
      "99%\n",
      "99%\n"
     ]
    }
   ],
   "source": [
    "# TF-IDF\n",
    "doc_term_matrix_tfidf = []\n",
    "\n",
    "counter = 0\n",
    "total = len(doclist)\n",
    "for tf_vector in doclist:\n",
    "    doc_term_matrix_tfidf.append(np.dot(tf_vector, my_idf_matrix))\n",
    "    counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        print str(100*counter/total) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0%\n",
      "1%\n",
      "1%\n",
      "2%\n",
      "3%\n",
      "3%\n",
      "4%\n",
      "5%\n",
      "5%\n",
      "6%\n",
      "7%\n",
      "7%\n",
      "8%\n",
      "9%\n",
      "9%\n",
      "10%\n",
      "11%\n",
      "11%\n",
      "12%\n",
      "13%\n",
      "13%\n",
      "14%\n",
      "15%\n",
      "15%\n",
      "16%\n",
      "17%\n",
      "17%\n",
      "18%\n",
      "19%\n",
      "19%\n",
      "20%\n",
      "21%\n",
      "21%\n",
      "22%\n",
      "23%\n",
      "23%\n",
      "24%\n",
      "24%\n",
      "25%\n",
      "26%\n",
      "26%\n",
      "27%\n",
      "28%\n",
      "28%\n",
      "29%\n",
      "30%\n",
      "30%\n",
      "31%\n",
      "32%\n",
      "32%\n",
      "33%\n",
      "34%\n",
      "34%\n",
      "35%\n",
      "36%\n",
      "36%\n",
      "37%\n",
      "38%\n",
      "38%\n",
      "39%\n",
      "40%\n",
      "40%\n",
      "41%\n",
      "42%\n",
      "42%\n",
      "43%\n",
      "44%\n",
      "44%\n",
      "45%\n",
      "46%\n",
      "46%\n",
      "47%\n",
      "47%\n",
      "48%\n",
      "49%\n",
      "49%\n",
      "50%\n",
      "51%\n",
      "51%\n",
      "52%\n",
      "53%\n",
      "53%\n",
      "54%\n",
      "55%\n",
      "55%\n",
      "56%\n",
      "57%\n",
      "57%\n",
      "58%\n",
      "59%\n",
      "59%\n",
      "60%\n",
      "61%\n",
      "61%\n",
      "62%\n",
      "63%\n",
      "63%\n",
      "64%\n",
      "65%\n",
      "65%\n",
      "66%\n",
      "67%\n",
      "67%\n",
      "68%\n",
      "69%\n",
      "69%\n",
      "70%\n",
      "71%\n",
      "71%\n",
      "72%\n",
      "72%\n",
      "73%\n",
      "74%\n",
      "74%\n",
      "75%\n",
      "76%\n",
      "76%\n",
      "77%\n",
      "78%\n",
      "78%\n",
      "79%\n",
      "80%\n",
      "80%\n",
      "81%\n",
      "82%\n",
      "82%\n",
      "83%\n",
      "84%\n",
      "84%\n",
      "85%\n",
      "86%\n",
      "86%\n",
      "87%\n",
      "88%\n",
      "88%\n",
      "89%\n",
      "90%\n",
      "90%\n",
      "91%\n",
      "92%\n",
      "92%\n",
      "93%\n",
      "94%\n",
      "94%\n",
      "95%\n",
      "95%\n",
      "96%\n",
      "97%\n",
      "97%\n",
      "98%\n",
      "99%\n",
      "99%\n"
     ]
    }
   ],
   "source": [
    "# normalize with L2 norm\n",
    "counter = 0\n",
    "total = len(doc_term_matrix_tfidf)\n",
    "doc_term_matrix_tfidf_l2 = []\n",
    "for tf_vector in doc_term_matrix_tfidf:\n",
    "    doc_term_matrix_tfidf_l2.append(l2_normalizer(tf_vector))\n",
    "    counter = counter + 1\n",
    "    if counter % 100 == 0:\n",
    "        print str(100*counter/total) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MultinomialNB(alpha=1.0, class_prior=None, fit_prior=True)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN THE MODEL\n",
    "X = np.array(doc_term_matrix_tfidf_l2)\n",
    "y = np.array([item[1] for item in freq_set])\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "clf = MultinomialNB()\n",
    "clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy over the training set: 71.9873775557%\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY OVER THE TRAINING SET\n",
    "a = clf.predict(doclist)\n",
    "count_match = 0\n",
    "count_tot = 0\n",
    "for item in freq_set:\n",
    "    if a[count_tot] == item[1]:\n",
    "        count_match = count_match + 1\n",
    "    count_tot = count_tot + 1\n",
    "print 'Accuracy over the training set: ' + str(100*float(count_match)/float(count_tot)) + '%'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.5714285714% of the Cocina casera images were well classified.\n",
      "99.6648044693% of the Tapas, pintxos y platillos images were well classified.\n",
      "7.35294117647% of the Paellas y arroces images were well classified.\n",
      "9.52380952381% of the Cocina rapida images were well classified.\n",
      "23.275862069% of the Cocina italiana images were well classified.\n",
      "41.6058394161% of the Cocina asiatica images were well classified.\n",
      "28.0575539568% of the Carnes a la brasa images were well classified.\n",
      "25.8620689655% of the Cocina dietetica, naturista, vegetariana y biologica images were well classified.\n",
      "Accuracy over the entire test set: 65.5397390273%.\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY OVER THE TEST SET\n",
    "for i in range(8):\n",
    "    a = clf.predict([img[0] for img in test_freq_set if img[1] == i])\n",
    "    count_match = 0\n",
    "    count_tot = 0\n",
    "    for item in [img for img in test_freq_set if img[1] == i]:\n",
    "        if a[count_tot] == item[1]:\n",
    "            count_match = count_match + 1\n",
    "        count_tot = count_tot + 1\n",
    "    print str(100*float(count_match)/float(count_tot)) + '% of the ' + rest_types[i] + ' images were well classified.'\n",
    "\n",
    "a = clf.predict([img[0] for img in test_freq_set])\n",
    "count_match = 0\n",
    "count_tot = 0\n",
    "for item in test_freq_set:\n",
    "    if a[count_tot] == item[1]:\n",
    "        count_match = count_match + 1\n",
    "    count_tot = count_tot + 1\n",
    "print 'Accuracy over the entire test set: ' + str(100*float(count_match)/float(count_tot)) + '%.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cocina casera: 0.0755525454506\n",
      "Tapas, pintxos y platillos: 0.0371862154177\n",
      "Paellas y arroces: 0.010996304745\n",
      "Cocina rapida: 0.71780172546\n",
      "Cocina italiana: 0.056856858131\n",
      "Cocina asiatica: 0.0681818685556\n",
      "Carnes a la brasa: 0.0251471642141\n",
      "Cocina dietetica, naturista, vegetariana y biologica: 0.00827731802632\n",
      "----------------------------------------------------------------------\n",
      "This picture should be Cocina rapida.\n"
     ]
    }
   ],
   "source": [
    "# GIVE ME A INSTAGRAM PICTURE URL (WITH ENOUGH HASHTAGS) AND I WILL TELL YOU WHICH IS ITS FOOD STYLE\n",
    "# THIS IS A PICTURE TAKEN IN A KFC, SO IT SHOULD SAY COCINA RAPIDA\n",
    "url = 'https://www.instagram.com/p/BHqpEI7DfaG/'\n",
    "try:\n",
    "    source = urllib2.urlopen(url)\n",
    "    htmlcode = source.read()\n",
    "    start = htmlcode.find('\"caption\":') + 12\n",
    "    end = htmlcode.find('\",',start)\n",
    "    hash_list = listHashtags(htmlcode[start:end])\n",
    "    freq_samp = [0]*len(dic)\n",
    "    for tag in hash_list:\n",
    "        try:\n",
    "            freq_samp[dic.index(str(tag))] = 1\n",
    "        except ValueError:\n",
    "            None\n",
    "    if sum(freq_samp) == 0:\n",
    "        print sum(freq_samp)\n",
    "        print 'I cannot classify this image because I do not understand its tags.'\n",
    "except URLError:\n",
    "    print 'URL Error'\n",
    "freq_samp = [freq_samp]\n",
    "clf.predict(freq_samp)\n",
    "for i in range(len(rest_types)):\n",
    "    caca = clf.predict_proba(freq_samp)[0]\n",
    "    print rest_types[i] + ': ' + str(caca[i])\n",
    "print '----------------------------------------------------------------------'\n",
    "print 'This picture should be ' + rest_types[clf.predict(freq_samp)[0]] + '.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "img_list_per_rest = []\n",
    "for rest in rest_list:\n",
    "    try:\n",
    "        only_img_list = []\n",
    "        for img in rest['IMAGES']:\n",
    "            if len(img['tags']) != 0:\n",
    "                only_img_list.append([tag.lower() for tag in img['tags']])\n",
    "        img_list_per_rest.append([only_img_list,rest['ESPECIALIDAD'],rest['NOMBRE']])\n",
    "    except KeyError:\n",
    "        None\n",
    "\n",
    "for img in img_list_per_rest:\n",
    "    img[1] = rest_types.index(img[1])\n",
    "for rest in img_list_per_rest:\n",
    "    if len(rest[0]) == 0:\n",
    "        img_list_per_rest.remove(rest)\n",
    "\n",
    "rests_to_remove = []\n",
    "for rest in img_list_per_rest:\n",
    "    suma = np.array([0]*8)\n",
    "    counter = 0\n",
    "    for img in rest[0]:\n",
    "        hash_appearing = [0]*len(dic)\n",
    "        for tag in img:\n",
    "            try:\n",
    "                hash_appearing[dic.index(str(tag))] = 1\n",
    "            except ValueError:\n",
    "                None\n",
    "        if sum(hash_appearing) != 0:\n",
    "            suma = suma + clf.predict_proba([hash_appearing])\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            None\n",
    "    if np.sum(suma) == 0:\n",
    "        rests_to_remove.append(rest)\n",
    "    else:\n",
    "        mean = suma/counter\n",
    "        rest.append((suma/counter).tolist()[0])\n",
    "for rest in rests_to_remove:\n",
    "    img_list_per_rest.remove(rest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55.3097345133% of the restaurants were well classified.\n",
      "Note that many images of those restaurants belong to the training set.\n"
     ]
    }
   ],
   "source": [
    "# ACCURACY OF THE RESTAURANT CLASSIFICATION\n",
    "counter_pos = 0\n",
    "counter = 0\n",
    "for rest in img_list_per_rest:\n",
    "    maxim = max(rest[3])\n",
    "    if rest[3].index(maxim) == rest[1]:\n",
    "        counter_pos = counter_pos + 1\n",
    "    counter = counter + 1\n",
    "print str(100*float(counter_pos)/counter) + '% of the restaurants were well classified.'\n",
    "print 'Note that many images of those restaurants belong to the training set.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# PLS SAVE\n",
    "save_obj(rest_types,'rest_types')\n",
    "save_obj(clf,'clf_'+str(max_n_of_images))\n",
    "save_obj(dic,'dic_'+str(max_n_of_images))\n",
    "save_obj(img_list_per_rest,'known_rests_probs_mean_'+str(max_n_of_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

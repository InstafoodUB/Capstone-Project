{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recomanador per tags\n",
    "\n",
    "Utilitzant el model Naive Bayes entrenat, donat un text amb tags d'Instagram relacionats amb menjar, et recomana un restaurant de l'estil a l'Eixample."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pickle\n",
    "import urllib2\n",
    "import numpy as np\n",
    "from urllib2 import URLError\n",
    "from bs4 import BeautifulSoup\n",
    "import selenium.webdriver as webdriver\n",
    "from selenium.common.exceptions import NoSuchElementException"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load and save lists and dictionaries with pickle\n",
    "def save_obj(obj, name ):\n",
    "    with open(''+ name + '.pkl', 'wb') as f:\n",
    "        pickle.dump(obj, f, pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def load_obj(name ):\n",
    "    with open('' + name + '.pkl', 'rb') as f:\n",
    "        return pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def restaurant_insta_scraping_location(url):\n",
    "    browser.get(url)\n",
    "    try:\n",
    "        browser.find_element_by_class_name('_oidfu').click()\n",
    "        more_than_21 = 1\n",
    "    except NoSuchElementException:\n",
    "        more_than_21 = 0\n",
    "    if more_than_21 == 1:\n",
    "        for i in range(20):\n",
    "            time.sleep(1.5)\n",
    "            browser.execute_script(\"window.scrollTo(0,document.body.scrollHeight);\") #Scroll down \n",
    "        time.sleep(1)\n",
    "        html=browser.page_source\n",
    "        try:\n",
    "            pictable = browser.find_element_by_xpath('/html/body/span/section/main/article/div[2]/div[1]')\n",
    "            is_there_pictable = 1\n",
    "        except NoSuchElementException:\n",
    "            is_there_pictable = 0\n",
    "        if is_there_pictable == 1:\n",
    "            rest_info = []\n",
    "            for element in pictable.find_elements_by_css_selector(\"a[class='_8mlbc _vbtk2 _t5r8b']\"):\n",
    "                img = element.find_element_by_class_name('_icyx7')\n",
    "                text = img.get_attribute('alt')\n",
    "                tags = [t.replace(\"#\",\"\") for t in re.findall(r'#\\w+', text)]\n",
    "                #dic['url'] = img.get_attribute('src').encode('ascii')\n",
    "                rest_info.append(tags)\n",
    "        else:\n",
    "            rest_info = 0\n",
    "            \n",
    "    else: \n",
    "        html=browser.page_source\n",
    "        try:\n",
    "            pictable = browser.find_element_by_xpath('/html/body/span/section/main/article/div[2]/div[1]')\n",
    "            is_there_pictable = 1\n",
    "        except:\n",
    "            is_there_pictable = 0\n",
    "        if is_there_pictable == 1:\n",
    "            rest_info = []\n",
    "            for element in pictable.find_elements_by_css_selector(\"a[class='_8mlbc _vbtk2 _t5r8b']\"):\n",
    "                img = element.find_element_by_class_name('_icyx7')\n",
    "                text = img.get_attribute('alt')\n",
    "                tags = [t.replace(\"#\",\"\") for t in re.findall(r'#\\w+', text)]\n",
    "                rest_info.append(tags)\n",
    "        else:\n",
    "            rest_info = 0\n",
    "    return rest_info\n",
    "\n",
    "def listHashtags(string): \n",
    "    hashtaglist = []\n",
    "    while string.find('#',0) != -1:\n",
    "        m = re.search('(?<=#)\\w+', string)\n",
    "        string = string.replace('#'+m.group(0),'')\n",
    "        hashtaglist.append(m.group(0))\n",
    "    hashtaglist = [item.lower() for item in hashtaglist]\n",
    "    return hashtaglist\n",
    "\n",
    "def image_insta_scraping(url):\n",
    "    try:\n",
    "        source = urllib2.urlopen(url)\n",
    "        htmlcode = source.read()\n",
    "        start = htmlcode.find('\"caption\":') + 12\n",
    "        end = htmlcode.find('\",',start)\n",
    "        hash_list = listHashtags(htmlcode[start:end])\n",
    "    except URLError:\n",
    "        hash_list = 0\n",
    "    return hash_list\n",
    "\n",
    "def get_freq_sample(tags,dic):\n",
    "    N = len(dic)\n",
    "    freq_sample = [0]*N\n",
    "    for tag in tags:\n",
    "        try:\n",
    "            freq_sample[dic.index(str(tag.lower()))] = 1\n",
    "        except ValueError:\n",
    "            None\n",
    "    return freq_sample \n",
    "\n",
    "def two_point_distance(point1,point2):\n",
    "    point1 = np.array(point1)\n",
    "    point2 = np.array(point2)\n",
    "    if len(point1.shape) != 1 or len(point2.shape) != 1 or point2.shape != point1.shape:\n",
    "        print '(two_point_distance) ERROR: enter two valid points '\n",
    "        return 'ERROR'\n",
    "    resta = point1 - point2\n",
    "    return np.sqrt(sum([el**2 for el in resta]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def tag_recommender(url,clf,dic,known_rests_probs_mean):\n",
    "    if 'locations' in url:\n",
    "        tags_per_image = restaurant_insta_scraping_location(url)\n",
    "        if tags_per_image == 0:\n",
    "            return 'ERROR: non-valid URL'\n",
    "        freq_samples_per_image = [get_freq_sample(tags,dic) for tags in tags_per_image]\n",
    "        old_freq_samples_per_image = freq_samples_per_image\n",
    "        freq_samples_per_image = [item for item in old_freq_samples_per_image if sum(item) != 0]\n",
    "        if len(freq_samples_per_image) == 0:\n",
    "            return 'ERROR: I cannot recommend this restaurant because I do not understand the tags of its images.'\n",
    "        pred_probs_per_image = [clf.predict_proba([freq_samp])[0] for freq_samp in freq_samples_per_image]\n",
    "        pred_probs_mean = (sum([np.array(pred_probs) for pred_probs in pred_probs_per_image])/len(pred_probs_per_image)).tolist()\n",
    "        distances = [two_point_distance(rest[3],pred_probs_mean) for rest in known_rests_probs_mean]\n",
    "        return known_rests_probs_mean[distances.index(min(distances))][2]\n",
    "    else:\n",
    "        if 'instagram.com' in url:\n",
    "            image_tags = image_insta_scraping(url)\n",
    "            if image_tags == 0:\n",
    "                return 'ERROR: non-valid URL'\n",
    "            freq_samp = get_freq_sample(image_tags,dic)\n",
    "            if sum(freq_samp) == 0:\n",
    "                print image_tags\n",
    "                return 'ERROR: I cannot recommend this image because I do not understand its tags.'\n",
    "            else:\n",
    "                pred_rest_type = clf.predict([freq_samp])\n",
    "                pred_probs = clf.predict_proba([freq_samp])[0].tolist()\n",
    "                distances = [two_point_distance(rest[3],pred_probs) for rest in known_rests_probs_mean]\n",
    "                return known_rests_probs_mean[distances.index(min(distances))][2]\n",
    "        else:\n",
    "            print 'ERROR: non-valid URL'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# we have, in this folder, classifiers trained with max max_n_of_images images per class.\n",
    "# for the moment max_n_of_images \\in {200,500,1000,2000,9000}\n",
    "max_n_of_images = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rest_list = load_obj('rest_list')\n",
    "clf = load_obj('clf_' + str(max_n_of_images))\n",
    "dic = load_obj('dic_' + str(max_n_of_images))\n",
    "known_rests_probs_mean = load_obj('known_rests_probs_mean_' + str(max_n_of_images))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the classifier accepts either instagram image urls or instagram location urls:\n",
    "#url = 'https://www.instagram.com/explore/locations/238741098/' # LA TAGLIATELLA (ITALIANO)\n",
    "#url = 'https://www.instagram.com/explore/locations/584346/' #YOSHINO (SUSHI)\n",
    "#url = 'https://www.instagram.com/explore/locations/375802654/' #POCAS FOTOS\n",
    "#url = 'https://www.instagram.com/p/BHqpEI7DfaG/' # KFC (FASTFOOD)\n",
    "#url = 'https://www.instagram.com/explore/locations/88101181/' # VEGGIE\n",
    "#url = 'https://www.instagram.com/p/BHkzkdmBO2d/' # VEGGIE PIC\n",
    "#url = 'https://www.instagram.com/p/BGkUS2JIMDg/' #VEGGIE PIC 2\n",
    "#url = 'https://www.instagram.com/p/BFsNyhQJgqq/' # DA GRECO (ITALIANO)\n",
    "#url = 'https://www.instagram.com/p/BHwqR2PhIlC/' # MAS PASTA\n",
    "#url = 'https://www.instagram.com/p/BCP_5NUKwPu/' # CARNE (JALEO)\n",
    "url = 'https://www.instagram.com/p/BHxoYqdBK6o/' #SUSHI\n",
    "#url = 'https://www.instagram.com/p/BHxoXXvj7J2/' #SUSHI\n",
    "\n",
    "path_to_chromedriver = '/Users/AlbertIribarne/Dropbox/UB/bigdata/instagram/chromedriver'\n",
    "browser = webdriver.Chrome(path_to_chromedriver)\n",
    "\n",
    "recommendation = tag_recommender(url,clf,dic,known_rests_probs_mean)\n",
    "print 'You shuld try: ' + recommendation\n",
    "\n",
    "# print more info than the name\n",
    "for rest in rest_list:\n",
    "    if rest['NOMBRE'] == recommendation:\n",
    "        print (rest['NOMBRE'],rest['COORDENADAS'],rest['ESPECIALIDAD'])\n",
    "\n",
    "browser.quit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
